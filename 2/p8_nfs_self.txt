문제 내용 :
1.server 또는 이외의 리눅스 운영체제를 사용하는 머신에서 1GB HDD를 추가하고 500MB 파티션을 생성한다
2. 1번에서 만든 파티션을 /nfs 디렉터리에 오토마운트한다.
3. 재부팅 후 mount 명령어로 nfs 연결하고 마운트 정보를 확인한다

문제 해결 :


.


해당 문제 해결 시나리오 에서는
VMware Pro 15.N 버전을 사용합니다.
아래의 상황을 그대로 따라하지 않고싶거나, 따라할 수 없는 상황에서는
'하드디스크의 추가'라는 부분만이라도 정확히 파악하고, 수행하면 됩니다.


'하드디스크의 추가'
VMware 의 Memory 를 눌러서 설정 창을 엽니다.
설정 창을 연 후, ADD 버튼을 눌러 하드디스크 하나를 더 추가해줄겁니다.
디스크 타입 등등 여러가지 부가적인 설정은 어떻게 되든 상관은 없습니다.
다만, 이번 문제에서는 '1GB HDD'라고 명시되어 있으니
용량을 맞추는 영역에서, 1GB를 맞춰주도록 합니다.

설정을 전부 마쳤으면, ok 를 눌러 설정을 저장하고, 가상머신을 다시 실행시킵니다.
sd 뒤에 a부터 a , b , c 순서로 추가적인 글자가 하나 더 있을텐데,
예를 들어
sda1 이라는 파티션이 있다면, 이는
sd 형식의 a니까 1번째의 1번 번호를 가진 파티션 이라는 뜻입니다.

예를 들어 이번에 추가된 하드디스크가
sdb 라는 이름을 부여받았다면,
sdb1 라는 이름으로 파티션을 만들어 줄 수 있다는 뜻입니다.


.


'파티션 생성'
이번에 수행해줄 작업은 파티션을 생성해준다는 중심 내용을 가지고 있습니다.

우선, 파티션을 만들어줄 물리적인 장치의 이름을 알아야 하는데
우리가 추가해준 장치는, 아직 아무런 파티션도 만들어주지 않은 상태일것이니
/dev 디렉터리로 들어가 sd 와 관련된 파일을 찾아봅니다.
그중에서, sd 뒤에 따라붙는 영어 철자가 가장 마지막인것이 우리가 방금 추가한 하드디스크일겁니다.
brw-rw----. 1 root disk 8,  0 Dec 20 21:38 sda
brw-rw----. 1 root disk 8,  1 Dec 20 21:38 sda1
brw-rw----. 1 root disk 8,  2 Dec 20 21:38 sda2
brw-rw----. 1 root disk 8,  3 Dec 20 21:38 sda3
brw-rw----. 1 root disk 8, 16 Dec 20 21:38 sdb
같은 파일들이 있는 상황이라면, 우리가 방금 추가해준 하드디스크는 sdb일 것이라는 이야기입니다.

이번 문제에서는 sdb 에서 작업을 수행합니다.

아무튼, 이제 우리가 파티션을 생성하고싶은 디스크(장치)가 무엇인지 확인을 하는 과정을 거쳤으니,
실질적으로 파티션을 만들어보도록 하겠습니다.

fdisk /dev/sdb
를 입력하여, sdb 장치의 파티션 설정을 시작하겠습니다.

Command (m for help) :
라는 구문이 나온다면

n
을 입력해서 새로운 파티션을 만들어주도록 하겠습니다.


Partition type:
    p   primary (0 primary, 0 extended, 4 free)
    e   extended
Select (default p) :
라는 구문이 나온다면 

p
를 입력해서 엔터를 치거나 아무것도 입력하지 않은 채로 엔터를 눌러서 Primary 파티션을 만들어주도록 하겠습니다.


Partition number (1-4, default 1) :
이라는 구문이 나온다면

1
이나, 아무것도 입력하지 않은 상태로 입력을 해서, 우리가 마운트해야할 파티션의 이름이 sdb1 이 되도록 만들어줍니다.


Last sector, +sectors on +size{K,M,G} (2048-2097151, default 2097151):
이라는 구문이 나온다면

+500M
을 입력하여 500 MB 크기의 파티션을 만들어주도록 하겠습니다.

w
를 입력하여 /dev/sdb 의 fdisk 작업을 종료합니다.

지금까지
500메가바이트의 크기를 가진, sdb1 이라는 이름의 파티션을 만들었습니다.

mkfs.xfs /dev/sdb1
을 입력해서, sdb1 이라는 이름을 가진 파티션을 xfs 라는 파일 시스템 형식으로써 포맷해주겠습니다.
이 명령어를 입력하지 않으면 '파일 시스템'이 반드시 주어져야하는 파티션의 입장에서 아무것도 할 수 없는 상태가 돼버립니다.
(*
[root@localhost dev]# mkfs.xfs /dev/sdb1
meta-data=/dev/sdb1              isize=512    agcount=4, agsize=32000 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=128000, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=855, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
이와같은 출력문은, 정상적으로 작동되고 있다는 뜻일겁니다.
)

우리는 최종적으로

/dev/sdb1 이라는 위치에 있는
500 메가바이트의 크기를 가진
xfs 라는 파일 시스템을 사용하는
파티션을 만들어줬습니다.

이제 오토마운트를 설정하겠습니다.


.


'오토마운트'

오토 마운트를 관리하는 파일은
/etc/fstab
입니다.

vi /etc/fstab
를 입력해서 vi 에디터로 fstab 을 열어주겠습니다.

해당 문서 말단에
/dev/sdb1   /nfs    xfs     default     0 0
을 입력해주고, 저장해주고, vi 에디터를 종료합니다.

위의 구문의 뜻은 다음과 같습니다.

/dev/sdb1       /nfs     xfs     defaults        0 0
장치명          : /dev/sdb1
마운트 포인트   : /nfs
장치의 fs type  : xfs
마운트 옵션     : default
dump 운용       : 0(안함)
부팅시 fsck 동작: 0(안함)

(*해당 디렉토리의 f3_fstab.txt 으로부터 발췌)



이제 이러한 자동적인 마운트가 정상적으로 작동하는지 확인하기 위해서 재시작하고, 정상작동을 확인해보겠습니다.

init 6
를 입력하여 머신을 다시 시작합니다.

다시 터미널로 돌아와서

df -h
를 입력합니다.

[root@localhost /]# df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        471M     0  471M   0% /dev
tmpfs           487M     0  487M   0% /dev/shm
tmpfs           487M  8.5M  478M   2% /run
tmpfs           487M     0  487M   0% /sys/fs/cgroup
/dev/sda3       8.7G  4.3G  4.5G  49% /
/dev/sdb1       497M   26M  472M   6% /nfs
/dev/sda1       297M  152M  145M  52% /boot
tmpfs            98M   20K   98M   1% /run/user/0

이와 똑같은 화면이 아니더라도 아래의
/dev/sdb1       497M   26M  472M   6% /nfs
이 행이 가장 중요합니다. 우리가 방금 설정해준거니까요.

/dev/sdb1 은 당연하게도 파티션의 이름일것이고
497M    은 전체 크기
26M     은 사용된 용량
472M    은 남은 용량,
6%      은 남은 용량 대비 사용된 용량의 퍼센트에이지일것입니다.
/nfs    는 마운트된 디렉터리입니다.

이제 nfs 서비스를 적용시키는 일이 남았습니다.


.


'nfs 서비스 와 방화벽'

우선, nfs 서비스에 관하여 방화벽을 설정해주도록 하겠습니다.

systemctl restart nfs
라고 입력하여 nfs를 (재)시작합니다.

systemctl status nfs
를 입력해서 중간에 'Active: active'라고 (초록색 글자로)출력되어있다면 문제없이 작동하는것입니다. 일단 이 이후의 작업을 하려면 이 확인절차는 반드시 필요합니다.

nfs가 정상적으로 작동하는것을 확인했으니, 방화벽을 설정하여 이 nfs서비스에 관한 포트는 예외로 둡니다.

firewall-cmd --permanent --add-service=nfs
를 입력하여 firewall(방화벽)을 통해 nfs서비스 통신을 할 수 있도록 방화벽 설정을 해줍니다.

firewall-cmd --reload
를 입력하여 firewall(방화벽)설정을 해준것을 reload(재시작)함으로써 다시 시작합니다.

이제 실질적으로 nfs 서비스를 설정해보겠습니다.


.


'nfs 서비스의 설정파일인 /etc/exports 설정'


vi /etc/exports
를 입력하여, client 에서 접속해서 client 의 로컬 디렉토리처럼 사용할 수 있게 설정해주도록 하겠습니다.
(https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=hanajava&logNo=220422771247 발췌)

해당 문서 말단에

/nfs    192.168.127.128(rw,no_root_squash,sync)
를 입력합니다. 의미는 아래와 같습니다.
/nfs    : 제공자 측에서 제공해줄 파티션이 마운트된 파일 위치


192.168.127.128 : 사용자 측의 아이피
rw      : 작업 형식. 일고, 쓰는것 모두를 허용한다는 내용이다.
no_root_squash : client의 root를 서버의 root로 매핑 (최상위 관리자 접속으로 시도하는 경우, 상대방의 최상위 관리자의 이름이 어떻게 되든지간에 '최상위 관리자'라는 것에 맞춰 연결해줌)
sync    : 파일 시스템이 변경되면 즉시 동기화

다만, 이번에 사용할 사용자는 '본인'입니다.
서버의 컴퓨터에서 nfs 를 스스로 제공하고, 스스로 사용한다는 컨셉의 문제입니다.

따라서, 제공자와 사용자의 아이피가 똑같은게 특징입니다.

위에서 사용된
192.168.127.128
이라는 주소를 입력하지 말고, 자신의 리눅스에서 사용되고 있는 IP 를 적어넣어야 합니다.

만약에 '됐었는데, 안됨'같은 종류의 문제가 있다면

mount, firewall, DHCP 의 문제일것입니다.
이중에서 'IPv4 주소'의 부분에서 발생될 수 있는 문제로써는
DHCP를 사용함으로써 이미 설정해놓은 exports에 적혀 있는 아이피와,
현재 nfs 서비스를 제공하는 서버의 아이피가 달라지는 상황이 있을 수 있습니다.

이는 DHCP의 사용을 중지하고, 고정된 아이피를 설정함으로써 해결할 수 있습니다.

아무튼, 이렇게 nfs 서비스를 다루는 파일인 /etc/exports 를 설정해줬습니다.


이제 서비스가 설정됐는지, 확인하는 작업을 거치겠습니다.


.


'nfs 서비스 정상 작동 확인 절차'

init 6
를 입력해서 시스템을 재시작해줍니다.

그리고, root의 권한을 가지고 제자리로 돌아온 후

exportfs -v
를 입력해서 현재 nfs 서비스를 진행하고 있는게 뭔지 살펴봅니다. 만약에 아무것도 뜨지 않은 경우에는 위의 과정을 다시 거쳐야합니다.
(*
[root@localhost ~]# exportfs -v
/nfs            192.168.127.128(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
)
이러한 비슷한 형식으로써 잘 출력되면 문제 없는것입니다.
이제 실질적으로 마운트를 하고, 제 기능을 하는지 확인해보겠습니다.


.



'nfs 서비스 작동'

mkdir /nfs_client
를 입력하여 클라이언트의 입장이 될 디렉터리 하나를 만들어주겠습니다.

mount -t nfs 192.168.127.128:/nfs /nfs_client
를 입력하여 마운트를 해줍니다.

cp /etc/inittab /nfs_client
같은 명령어 이외에도, 안에 무언가 들어갔음을 증명할 수 있는 파일 하나를 대충 만들어서 /nfs_client 안에 넣어주면 됩니다.

ls -l /nfs_client
를 입력해줍니다. 정상적으로
(*
[root@localhost /]# ls -l nfs_client/
total 4
-rw-r--r-- 1 root root 511 Dec 21 15:34 inittab
)
같은 구문이 출력된다면, 파일 복붙까지는 정상적으로 된것입니다.

이제 핵심입니다.

우리는 /nfs에 무언가 복붙해넣지도, 무언가 작성하지도 않았습니다.
만약에, /nfs 안에 무언가 들어있다면
우리가 앞에서 설정해준 nfs 서비스에 대한 모든 것들이 정상적으로 작동하고있음을 증명할것입니다.

결과는 다음과 같습니다.


[root@localhost /]# ls -l /nfs
total 4
-rw-r--r-- 1 root root 511 Dec 21 15:34 inittab

정상적으로 출력되는것을 확인할 수 있습니다!

수고하셨습니다.